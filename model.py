from flask import Flask, request, jsonify, render_template
from flask_cors import CORS  
import os
import shutil
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env
load_dotenv()

# üîë ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Token ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
HF_TOKEN = os.getenv("HF_TOKEN")
HUGGINGFACE_REPO_ID = "mistralai/Mistral-Small-24B-Instruct-2501"
DB_FAISS_PATH = "vectorstore/db_faiss"
DATA_PATH = "The_Python_Book.pdf"

# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Flask ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î CORS
app = Flask(__name__, static_folder="static", template_folder="templates")
CORS(app)

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å PDF
def load_pdf(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF: {file_path}")
    
    loader = PyMuPDFLoader(file_path)
    return loader.load()

documents = load_pdf(DATA_PATH)

# ‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)
text_chunks = splitter.split_documents(documents)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"trust_remote_code": True},
    encode_kwargs={"normalize_embeddings": True}
)

# ‚úÖ ‡∏•‡∏ö FAISS Index ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏°‡∏≠
if os.path.exists(DB_FAISS_PATH):
    shutil.rmtree(DB_FAISS_PATH, ignore_errors=True)
os.makedirs(DB_FAISS_PATH, exist_ok=True)

db = FAISS.from_documents(text_chunks, embedding_model)
db.save_local(DB_FAISS_PATH)

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏ú‡πà‡∏≤‡∏ô InferenceClient
client = InferenceClient(api_key=HF_TOKEN)

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
def generate_response(query):
    context_docs = db.similarity_search_with_score(query, k=3)  
    if not context_docs or context_docs[0][1] < 0.5:  
        return "I don't know. The answer is not found in the provided document."

    combined_context = "\n\n".join([doc.page_content for doc, score in context_docs])

    refined_prompt = f"""
    You are an AI assistant that strictly answers questions based on the given document.
    Do not make up information. If the answer is not found in the document, please say so.
    If the answer is found in the document, provide the answer with the relevant context.
    If the answer is not found in the document,try to guess the answer based on the context.
    If you are not sure, please say so.

    Context:
    {combined_context}

    Question:
    {query}

    Answer:
    """

    result = client.text_generation(prompt=refined_prompt, max_new_tokens=100, temperature=0.1)
    return result

# ‚úÖ ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå HTML (Frontend)
@app.route("/")
def index():
    return render_template("index.html")  # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ô /templates

# ‚úÖ API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å Frontend
@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        query = data.get("question", "").strip()

        if not query:
            return jsonify({"error": "No question provided"}), 400

        response = generate_response(query)
        return jsonify({"answer": response})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)
